# CoreChain System Workflow & Theory

## ğŸ¯ What CoreChain Does (In Theory)

CoreChain is a **privacy-preserving collaborative AI platform** that allows multiple hospitals to train a shared TB detection model WITHOUT sharing their patient data.

### The Problem It Solves
- Hospitals have valuable medical data but can't share it due to **HIPAA/privacy laws**
- Individual hospitals don't have enough data to train accurate AI models
- No trust mechanism to ensure fair contribution and credit

### The Solution
CoreChain combines **3 technologies**:
1. **Federated Learning** - Train together without sharing data
2. **Homomorphic Encryption** - Protect model updates
3. **Blockchain** - Transparent audit trail and rewards

---

## ğŸ”„ Complete System Workflow

### Phase 1: Setup (One-Time)

**Central Aggregator (Your Main Laptop):**
```bash
docker pull saadhaniftaj/corechain-allinone:latest
docker run -d -p 80:80 -p 8080:8080 -p 50051:50051 \
  --name corechain-aggregator \
  saadhaniftaj/corechain-allinone:latest
```

**What Starts:**
- âœ… Flower Server (port 8080) - Coordinates federated learning
- âœ… REST API (port 8000) - Provides data to dashboard
- âœ… WebSocket Server (port 8001) - Real-time updates
- âœ… Blockchain Service (port 7050) - Immutable ledger
- âœ… Dashboard (port 80) - Web interface
- âœ… gRPC Server (port 50051) - Hospital communication

**Hospital Nodes (Other Laptops):**
```bash
# Each hospital runs their own node
docker run -d \
  -e HOSPITAL_ID=hospital_1 \
  -e HOSPITAL_NAME="General Hospital" \
  -e AGGREGATOR_IP=192.168.1.100 \
  -e DATASET_TYPE=shenzhen \
  --name hospital-node \
  saadhaniftaj/corechain-hospital:latest
```

---

### Phase 2: Training Workflow (Automatic)

#### Round 1 Begins:

**Step 1: Hospital Registration**
```
Hospital 1 â†’ gRPC â†’ Aggregator: "I'm Hospital 1, I have 500 images"
Hospital 2 â†’ gRPC â†’ Aggregator: "I'm Hospital 2, I have 300 images"
Hospital 3 â†’ gRPC â†’ Aggregator: "I'm Hospital 3, I have 400 images"
```

**Step 2: Local Training (Parallel)**
```
Hospital 1: Trains CNN on local 500 images for 5 epochs
Hospital 2: Trains CNN on local 300 images for 5 epochs
Hospital 3: Trains CNN on local 400 images for 5 epochs
```

Each hospital:
- Loads their local TB X-ray images
- Trains the model locally (data NEVER leaves their machine)
- Achieves local accuracy (e.g., 72%, 68%, 75%)

**Step 3: Encrypt Model Updates**
```
Hospital 1: model_weights â†’ Paillier Encryption â†’ encrypted_weights
Hospital 2: model_weights â†’ Paillier Encryption â†’ encrypted_weights
Hospital 3: model_weights â†’ Paillier Encryption â†’ encrypted_weights
```

**Step 4: Send to Aggregator**
```
Hospital 1 â†’ gRPC â†’ Aggregator: encrypted_weights + metadata
Hospital 2 â†’ gRPC â†’ Aggregator: encrypted_weights + metadata
Hospital 3 â†’ gRPC â†’ Aggregator: encrypted_weights + metadata
```

Metadata includes:
- Number of samples trained on
- Local accuracy achieved
- Hospital ID

**Step 5: Aggregation (FedAvg)**
```
Aggregator:
  1. Decrypts all model updates
  2. Performs weighted averaging:
     global_weights = (500*w1 + 300*w2 + 400*w3) / 1200
  3. Creates new global model
```

**Step 6: Blockchain Logging**
```
Blockchain â† Transaction: {
  type: "MODEL_UPDATE",
  hospital: "hospital_1",
  round: 1,
  accuracy: 0.72,
  samples: 500
}

Blockchain â† Transaction: {
  type: "MODEL_AGGREGATION",
  round: 1,
  global_accuracy: 0.73,
  participants: 3
}
```

**Step 7: Reward Distribution**
```
Smart Contract Calculates:
  Hospital 1: base(10) + accuracy_bonus(3.6) + contribution(2.1) = 15.7 tokens
  Hospital 2: base(10) + accuracy_bonus(3.4) + contribution(1.3) = 14.7 tokens
  Hospital 3: base(10) + accuracy_bonus(3.75) + contribution(1.7) = 15.45 tokens

Blockchain â† Reward Transactions
```

**Step 8: Distribute Global Model**
```
Aggregator â†’ gRPC â†’ Hospital 1: global_model_weights
Aggregator â†’ gRPC â†’ Hospital 2: global_model_weights
Aggregator â†’ gRPC â†’ Hospital 3: global_model_weights
```

**Step 9: Dashboard Update**
```
WebSocket â†’ Dashboard: {
  event: "round_complete",
  round: 1,
  accuracy: 0.73,
  participants: 3
}

Dashboard displays:
  - Progress: Round 1/10 (10%)
  - Global Accuracy: 73%
  - Connected Hospitals: 3
  - Recent blockchain transactions
```

#### Rounds 2-10: Repeat

Each round:
- Hospitals start with the improved global model
- Train on their local data
- Model gets better each round
- Final accuracy: ~85-90% (vs 72% individual)

---

## ğŸ“Š What's Working (In Theory)

### âœ… Fully Implemented Components:

**1. Federated Learning Layer**
- âœ… Flower server with custom FedAvg strategy
- âœ… Flower client for hospital nodes
- âœ… Weighted aggregation based on dataset size
- âœ… Model distribution and updates
- âœ… TB detection CNN (4 conv blocks, batch norm, dropout)

**2. Privacy Layer**
- âœ… Paillier homomorphic encryption
- âœ… Gradient encryption/decryption
- âœ… Homomorphic aggregation (add encrypted values)
- âœ… Key generation and management

**3. Blockchain Layer**
- âœ… Lightweight blockchain (SHA-256, PoW)
- âœ… Smart contracts (validator, reward distributor, audit logger)
- âœ… Transaction pool and mining
- âœ… Chain validation
- âœ… RESTful API (Fabric-compatible)

**4. Communication Layer**
- âœ… gRPC for model updates (Protocol Buffers)
- âœ… REST API for dashboard queries
- âœ… WebSocket for real-time updates
- âœ… All services containerized

**5. Dashboard**
- âœ… Real-time training status
- âœ… Connected hospitals viewer
- âœ… Blockchain statistics
- âœ… Transaction history
- âœ… Auto-refresh (5 seconds)
- âœ… Responsive design with animations

**6. Data Pipeline**
- âœ… TB dataset loader (Shenzhen/Montgomery)
- âœ… Synthetic data generation (for demos)
- âœ… Data preprocessing and augmentation
- âœ… Train/test splitting

---

## ğŸ¬ Expected Demo Workflow

### Setup (5 minutes):
1. **Aggregator Laptop**: Run Docker container
2. **Hospital Laptops**: Run hospital nodes (3 laptops)
3. **Browser**: Open dashboard at http://aggregator-ip

### Demo (10 minutes):

**Minute 1-2: Introduction**
- Show the 4 laptops
- Explain the problem (privacy vs collaboration)
- Show dashboard landing page

**Minute 3-4: Start Training**
```bash
# On each hospital laptop
docker logs -f hospital-node
```
- Show logs: "Connecting to aggregator..."
- Show logs: "Registered successfully"
- Show logs: "Starting Round 1..."

**Minute 5-6: Watch Training**
- Dashboard shows: "Round 1/10 - Training Active"
- Watch accuracy increase: 72% â†’ 75% â†’ 78%
- Point out: "Data never leaves each hospital"

**Minute 7-8: Blockchain Transparency**
```bash
curl http://aggregator-ip/blockchain/api/blockchain/chain
```
- Show blockchain transactions
- Show reward distribution
- Explain: "Immutable audit trail"

**Minute 9: Results**
- Final accuracy: ~85%
- Show leaderboard
- Compare: Individual (72%) vs Collaborative (85%)

**Minute 10: Impact**
- Explain real-world applications
- Discuss scalability
- Q&A

---

## ğŸ”§ Technical Flow (Behind the Scenes)

### When You Run the Container:

```
1. Supervisor starts all services:
   â”œâ”€ Blockchain (Python FastAPI)
   â”œâ”€ Aggregator (Python Flower + gRPC + REST + WebSocket)
   â””â”€ Nginx (Serves dashboard + proxies APIs)

2. Blockchain initializes:
   - Creates genesis block
   - Starts mining thread
   - Opens REST API on port 7050

3. Aggregator initializes:
   - Connects to blockchain
   - Starts Flower server (waits for MIN_CLIENTS)
   - Opens gRPC server (port 50051)
   - Opens REST API (port 8000)
   - Opens WebSocket (port 8001)

4. Dashboard loads:
   - Nginx serves index.html
   - JavaScript polls REST API every 5 seconds
   - WebSocket connects for real-time updates
   - Shows "Waiting for hospitals..."

5. Hospital connects:
   - gRPC call: register(hospital_id, name, samples)
   - Aggregator: "Hospital registered"
   - Dashboard updates: "Connected Hospitals: 1"

6. When MIN_CLIENTS reached:
   - Flower server: "Starting federated learning"
   - Round 1 begins automatically
   - Hospitals train in parallel
   - Aggregation happens
   - Blockchain logs everything
   - Dashboard shows progress

7. After 10 rounds:
   - Training complete
   - Final model saved
   - Blockchain has full audit trail
   - Dashboard shows "Complete" status
```

---

## ğŸ¯ Success Metrics

**What Should Work:**
1. âœ… Container starts all services
2. âœ… Dashboard loads and shows UI
3. âœ… Hospital nodes can connect
4. âœ… Training rounds execute
5. âœ… Blockchain records transactions
6. âœ… Dashboard updates in real-time
7. âœ… Final model achieves >80% accuracy

**What to Expect:**
- **Setup time**: 2-3 minutes
- **Per round**: 30-60 seconds (depending on data size)
- **Total training**: 5-10 minutes for 10 rounds
- **Blockchain size**: ~50-100 transactions
- **Memory usage**: ~2GB per container

---

## ğŸš€ Production Readiness

**What's Production-Ready:**
- âœ… Dockerized architecture
- âœ… Multi-service orchestration
- âœ… Error handling and logging
- âœ… Health checks
- âœ… Auto-restart on failure

**What Needs Work for Production:**
- âš ï¸ Authentication (JWT tokens)
- âš ï¸ HTTPS/TLS encryption
- âš ï¸ Database persistence (PostgreSQL)
- âš ï¸ Horizontal scaling
- âš ï¸ Full homomorphic encryption
- âš ï¸ Real TB datasets (HIPAA compliance)

---

## ğŸ“ Summary

**CoreChain is a COMPLETE PROTOTYPE that demonstrates:**
1. Multi-hospital collaboration without data sharing
2. Privacy through encryption
3. Transparency through blockchain
4. Real-time monitoring through dashboard
5. Fair reward distribution

**It's ready for:**
- âœ… Academic presentations
- âœ… Proof-of-concept demos
- âœ… Research papers
- âœ… Hackathon submissions

**Next steps for real deployment:**
- Add authentication
- Use real medical datasets
- Deploy to cloud (Azure/AWS)
- Integrate with hospital systems
- Get regulatory approval (HIPAA/GDPR)

---

**The Docker container is building now. Once complete, you'll be able to:**
```bash
docker pull saadhaniftaj/corechain-allinone:latest
docker run -d -p 80:80 -p 8080:8080 -p 50051:50051 saadhaniftaj/corechain-allinone:latest
```

**And your entire system will be running!** ğŸ‰
